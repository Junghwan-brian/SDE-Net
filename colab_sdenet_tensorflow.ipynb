{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sdenet_tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1tkXlV6EXqAik_Oj1taBRm0Wsnc0NUnU6",
      "authorship_tag": "ABX9TyMNuLfZ11VwMpS4Sqgw5fb9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junghwan-brian/SDE-Net/blob/master/colab_sdenet_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbDi-MwdBQIi",
        "outputId": "ba074d88-28a3-4e93-c25f-a759352a78f4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Nov 18 12:30:37 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEvnki3HBWPP"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from sklearn import datasets\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from warnings import filterwarnings\n",
        "import random\n",
        "import math\n",
        "from __future__ import print_function\n",
        "import time\n",
        "from scipy import misc\n",
        "from numpy.linalg import inv\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, ReLU, Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "filterwarnings(\"ignore\")\n",
        "def load_MSD():\n",
        "# Load the raw data.\n",
        "    num_attributes = 90\n",
        "    names = ['Year'] + ['Attr_{}'.format(i) for i in range(num_attributes)]\n",
        "    # YearPredictionMSD load file\n",
        "    df = pd.read_csv('drive/MyDrive/SDE-Net/YearPredictionMSD.txt', header=None, names=names)\n",
        "\n",
        "# Validate the data.\n",
        "    num_examples = 515345\n",
        "    assert len(df.columns) == num_attributes + 1\n",
        "    assert len(df) == num_examples\n",
        "    assert not df.isnull().values.any()\n",
        "\n",
        "\n",
        "# Train/test split. See \"Data Set Information\".\n",
        "    num_train = 463715\n",
        "    df = df.values\n",
        "    train = df[:num_train]\n",
        "    test = df[num_train:]\n",
        "    del df\n",
        "\n",
        "\n",
        "# Seperate inputs and outputs.\n",
        "    X_train, y_train = train[:, 1:], train[:, 0]\n",
        "    X_test, y_test = test[:, 1:], test[:, 0]\n",
        "    del train\n",
        "    del test\n",
        "    \n",
        "    standardize = StandardScaler().fit(X_train)\n",
        "    X_train = standardize.transform(X_train)\n",
        "    X_test = standardize.transform(X_test)\n",
        "\n",
        "    y_train1 = np.expand_dims(y_train, axis=1)\n",
        "    y_test1 = np.expand_dims(y_test , axis=1)\n",
        "\n",
        "    standardize2 = StandardScaler().fit(y_train1)\n",
        "    y_train = standardize2.transform(y_train1)\n",
        "    y_train = np.squeeze(y_train)\n",
        "\n",
        "    y_test1 = np.expand_dims(y_test, axis=1)\n",
        "    y_test = standardize2.transform(y_test1)\n",
        "    y_test = np.squeeze(y_test)\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def load_boston():\n",
        "    boston = datasets.load_boston()\n",
        "    x = boston.data\n",
        "    X = np.concatenate((x,x), axis=1)\n",
        "    for i in range(5):\n",
        "        X = np.concatenate((X,x), axis=1)\n",
        "    X = X[:,0:-1]\n",
        "    standardize = StandardScaler().fit(X)\n",
        "    X = standardize.transform(X)\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(dataset):\n",
        "    if dataset == 'MSD':\n",
        "        X_train, y_train, X_test, y_test = load_MSD()\n",
        "        return X_train, y_train, X_test, y_test\n",
        "    if dataset == 'boston':\n",
        "        x = load_boston()\n",
        "        return x\n",
        "    \n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-jNGhSTBXle"
      },
      "source": [
        "tf.keras.backend.set_floatx(\"float64\")\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "\n",
        "class Drift(Layer):\n",
        "    def __init__(self):\n",
        "        super(Drift, self).__init__(name=\"drift_net\")\n",
        "        self.fc = Dense(50, kernel_initializer=\"he_normal\",activation = 'relu')  # input : 50\n",
        "\n",
        "    def call(self, t, x):\n",
        "        out = self.fc(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Diffusion(Layer):\n",
        "    def __init__(self):\n",
        "        super(Diffusion, self).__init__(name=\"diffusion_net\")\n",
        "        self.fc1 = Dense(100, kernel_initializer=\"he_normal\",activation = 'relu')  # input : 50\n",
        "        self.fc2 = Dense(1)  # input : 100\n",
        "\n",
        "    def call(self, t, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.fc2(out)\n",
        "        out = tf.nn.sigmoid(out)\n",
        "        return out  # batch,1\n",
        "\n",
        "\n",
        "class SDENet(Model):\n",
        "    def __init__(self, layer_depth):\n",
        "        super(SDENet, self).__init__(name=\"SDE_Net\")\n",
        "        self.layer_depth = layer_depth\n",
        "        self.downsampling_layers = Dense(\n",
        "            50, kernel_initializer=\"he_normal\"\n",
        "        )  # batch, 50\n",
        "        self.drift = Drift()  # batch, 1\n",
        "        self.diffusion = Diffusion()\n",
        "        self.fc_layers = Sequential(\n",
        "            [ReLU(), Dense(2)]\n",
        "        )  # input : 50, output : mean, variance\n",
        "        self.deltat = 4.0 / self.layer_depth  # T:4, N:layer_depth\n",
        "        self.sigma_max = 0.5  # sigma_max : scaling diffusion output\n",
        "\n",
        "    def call(self, x, training_diffusion=False):\n",
        "        out = self.downsampling_layers(x)\n",
        "        if not training_diffusion:\n",
        "            t = 0\n",
        "            diffusion_term = self.sigma_max * self.diffusion(t, out)\n",
        "            for i in range(self.layer_depth):\n",
        "                t = 4 * (float(i)) / self.layer_depth\n",
        "                out = (\n",
        "                    out\n",
        "                    + self.drift(t, out) * self.deltat\n",
        "                    + diffusion_term\n",
        "                    * tf.cast(tf.math.sqrt(self.deltat), \"float64\")\n",
        "                    * tf.random.normal(tf.shape(out), dtype=\"float64\")\n",
        "                )  # Euler-Maruyama method\n",
        "\n",
        "            final_out = self.fc_layers(out)\n",
        "            mean = final_out[:, 0]\n",
        "            # sigma should be greater than 0.\n",
        "            sigma = tf.math.softplus(final_out[:, 1]) + 1e-3\n",
        "            return mean, sigma\n",
        "\n",
        "        else:\n",
        "            t = 0\n",
        "            final_out = self.diffusion(t, out)\n",
        "            return final_out\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPl1RhlKBpXF",
        "outputId": "a1085024-00b6-4e79-f53d-6bf66ad310dd"
      },
      "source": [
        "\n",
        "filterwarnings(\"ignore\")\n",
        "epochs = 1\n",
        "lr = 1e-4\n",
        "lr2 = 0.01\n",
        "seed = 0\n",
        "batch_size = 128\n",
        "target_scale = 10.939756\n",
        "# Data\n",
        "print(\"==> Preparing data..\")\n",
        "\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_dataset(\"MSD\")\n",
        "\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "    .batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "test_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "    .batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Model\n",
        "print(\"==> Building model..\")\n",
        "model = SDENet(4)\n",
        "\n",
        "real_label = 0  # training data\n",
        "fake_label = 1  # out-of-distribution data\n",
        "\n",
        "criterion = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "\n",
        "# optimize drift net\n",
        "optimizer_drift = tfa.optimizers.SGDW(learning_rate=lr, weight_decay=5e-4,)\n",
        "\n",
        "# optimize fc layer\n",
        "optimizer_fc = tfa.optimizers.SGDW(learning_rate=lr, weight_decay=5e-4,)\n",
        "\n",
        "# optimize down sampling layer\n",
        "optimizer_dsl = tfa.optimizers.SGDW(learning_rate=lr, weight_decay=5e-4,)\n",
        "\n",
        "# optimize diffusion net\n",
        "optimizer_diffusion = tfa.optimizers.SGDW(\n",
        "    learning_rate=lr2, weight_decay=5e-4,\n",
        ")\n",
        "\n",
        "\n",
        "def nll_loss(y, mean, sigma):\n",
        "    loss = tf.math.reduce_mean(tf.math.log(sigma ** 2) + (y - mean) ** 2 / (sigma ** 2))\n",
        "    return loss\n",
        "\n",
        "\n",
        "mse = tf.keras.losses.MSE\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "test_loss = tf.keras.metrics.Mean()\n",
        "in_loss = tf.keras.metrics.Mean()\n",
        "out_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_net(x, y):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        mean, sigma = model(x, training_diffusion=False)\n",
        "        loss = nll_loss(y, mean, sigma)\n",
        "\n",
        "    drift_gradient = tape.gradient(loss, model.drift.trainable_variables)\n",
        "    dsl_gradient = tape.gradient(loss, model.downsampling_layers.trainable_variables)\n",
        "    fc_gradient = tape.gradient(loss, model.fc_layers.trainable_variables)\n",
        "\n",
        "    drift_gradient = [(tf.clip_by_norm(grad, 100)) for grad in drift_gradient]\n",
        "    dsl_gradient = [(tf.clip_by_norm(grad, 100)) for grad in dsl_gradient]\n",
        "    fc_gradient = [(tf.clip_by_norm(grad, 100)) for grad in fc_gradient]\n",
        "\n",
        "    optimizer_drift.apply_gradients(\n",
        "        zip(drift_gradient, model.drift.trainable_variables)\n",
        "    )\n",
        "    optimizer_dsl.apply_gradients(\n",
        "        zip(dsl_gradient, model.downsampling_layers.trainable_variables)\n",
        "    )\n",
        "    optimizer_fc.apply_gradients(zip(fc_gradient, model.fc_layers.trainable_variables))\n",
        "    train_loss(loss)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_diffusion(real_x):\n",
        "    with tf.GradientTape(watch_accessed_variables=False) as real_tape_diffusion:\n",
        "        # only access to diffusion layer's parameters\n",
        "        real_tape_diffusion.watch(model.diffusion.trainable_variables)\n",
        "        real_y = tf.fill((real_x.shape[0], 1), real_label)\n",
        "        real_pred = model(real_x, training_diffusion=True)\n",
        "        real_loss = mse(real_y, real_pred)\n",
        "\n",
        "    diffusion_gradient = real_tape_diffusion.gradient(\n",
        "        real_loss, model.diffusion.trainable_variables\n",
        "    )\n",
        "\n",
        "    diffusion_gradient1 = [(tf.clip_by_norm(grad, 100)) for grad in diffusion_gradient]\n",
        "\n",
        "    with tf.GradientTape(watch_accessed_variables=False) as fake_tape_diffusion:\n",
        "        fake_tape_diffusion.watch(model.diffusion.trainable_variables)\n",
        "        # fake std is 2 in official code, but in paper it is 4\n",
        "        fake_x = (\n",
        "            tf.cast(\n",
        "                tf.random.normal((real_x.shape[0], 90), mean=0, stddev=2), \"float64\"\n",
        "            )\n",
        "            + real_x\n",
        "        )\n",
        "        fake_y = tf.fill((real_x.shape[0], 1), fake_label)\n",
        "        fake_pred = model(fake_x, training_diffusion=True)\n",
        "        fake_loss = mse(fake_y, fake_pred)\n",
        "\n",
        "    diffusion_gradient = fake_tape_diffusion.gradient(\n",
        "        fake_loss, model.diffusion.trainable_variables\n",
        "    )\n",
        "\n",
        "    diffusion_gradient2 = [(tf.clip_by_norm(grad, 100)) for grad in diffusion_gradient]\n",
        "\n",
        "    diffusion_gradient = [\n",
        "        (grad1 + grad2) for grad1, grad2 in zip(diffusion_gradient1, diffusion_gradient2)\n",
        "    ]\n",
        "\n",
        "    optimizer_diffusion.apply_gradients(\n",
        "        zip(diffusion_gradient, model.diffusion.trainable_variables)\n",
        "    )\n",
        "    in_loss(real_loss)\n",
        "    out_loss(fake_loss)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def test_net(x, y):\n",
        "    current_mean = 0\n",
        "    for i in range(10):\n",
        "        mean, sigma = model(x, training_diffusion=False)\n",
        "        current_mean += mean\n",
        "    current_mean = current_mean / 10\n",
        "    loss = mse(y, current_mean) * target_scale\n",
        "\n",
        "    test_loss(loss)\n",
        "\n",
        "# Training\n",
        "def train():\n",
        "\n",
        "    for data, label in train_ds:\n",
        "        train_net(data, label)\n",
        "        train_diffusion(data)\n",
        "\n",
        "    print(\n",
        "        f\"Loss: {train_loss.result():.4f}, Loss_in: {in_loss.result():.4f}, Loss_out: {out_loss.result():.4f}\"\n",
        "    )\n",
        "    train_loss.reset_states()\n",
        "    in_loss.reset_states()\n",
        "    out_loss.reset_states()\n",
        "\n",
        "\n",
        "# Testing\n",
        "def test():\n",
        "    for data, label in test_ds:\n",
        "        test_net(data, label)\n",
        "    print(f\"Test Loss : {test_loss.result():.4f}\")\n",
        "    test_loss.reset_states()\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch: {epoch + 1}\")\n",
        "\n",
        "    if epoch == 0:\n",
        "        # In the thesis, initial sigma_max is 0.01 but in the author's code it is 0.1\n",
        "        model.sigma_max = 0.1\n",
        "    if epoch == 30:\n",
        "        model.sigma_max = 0.5\n",
        "    train()\n",
        "    test()\n",
        "\n",
        "path = \"sde_net.h5\"\n",
        "tf.saved_model.save(model, path)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "==> Building model..\n",
            "\n",
            "Epoch: 1\n",
            "WARNING:tensorflow:Layer drift_net is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Loss: 78975.9072, Loss_in: 0.0195, Loss_out: 0.0126\n",
            "Test Loss : 10.6518\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: sde_net.h5/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElIFDuv81KOF"
      },
      "source": [
        "# Training settings\n",
        "eval_iter = 10\n",
        "seed = 0\n",
        "gpu = 0\n",
        "outf = \"test_sde\"\n",
        "batch_size = 512\n",
        "target_scale = 10.939756\n",
        "\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "in_test_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "    .batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "\n",
        "X_out = load_dataset(\"boston\")\n",
        "\n",
        "os.makedirs(outf,exist_ok=True)\n",
        "# then make file : confidence_Base_In.txt, confidence_Base_Out.txt\n",
        "\n",
        "\n",
        "def tpr95(dir_name):\n",
        "    # calculate the falsepositive error when tpr is 95%\n",
        "    cifar = np.loadtxt(\"%s/confidence_Base_In.txt\" % dir_name, delimiter=\",\")\n",
        "    other = np.loadtxt(\"%s/confidence_Base_Out.txt\" % dir_name, delimiter=\",\")\n",
        "\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1), np.min(Y1)])\n",
        "    gap = (end - start) / 200000  # precision:200000\n",
        "\n",
        "    total = 0.0\n",
        "    fpr = 0.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tpr = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
        "        error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
        "        if tpr <= 0.96 and tpr >= 0.94:\n",
        "            fpr += error2\n",
        "            total += 1\n",
        "    if total == 0:\n",
        "        print(\"corner case\")\n",
        "        fprBase = 1\n",
        "    else:\n",
        "        fprBase = fpr / total\n",
        "\n",
        "    return fprBase\n",
        "\n",
        "\n",
        "def auroc(dir_name):\n",
        "    # calculate the AUROC\n",
        "    f1 = open(\"%s/Update_Base_ROC_tpr.txt\" % dir_name, \"w\")\n",
        "    f2 = open(\"%s/Update_Base_ROC_fpr.txt\" % dir_name, \"w\")\n",
        "    cifar = np.loadtxt(\"%s/confidence_Base_In.txt\" % dir_name, delimiter=\",\")\n",
        "    other = np.loadtxt(\"%s/confidence_Base_Out.txt\" % dir_name, delimiter=\",\")\n",
        "\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1), np.min(Y1)])\n",
        "    gap = (end - start) / 200000\n",
        "\n",
        "    aurocBase = 0.0\n",
        "    fprTemp = 1.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tpr = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
        "        fpr = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
        "        f1.write(\"{}\\n\".format(tpr))\n",
        "        f2.write(\"{}\\n\".format(fpr))\n",
        "        aurocBase += (-fpr + fprTemp) * tpr\n",
        "        fprTemp = fpr\n",
        "    f1.close()\n",
        "    f2.close()\n",
        "    return aurocBase\n",
        "\n",
        "\n",
        "def auprIn(dir_name):\n",
        "    # calculate the AUPR\n",
        "    cifar = np.loadtxt(\"%s/confidence_Base_In.txt\" % dir_name, delimiter=\",\")\n",
        "    other = np.loadtxt(\"%s/confidence_Base_Out.txt\" % dir_name, delimiter=\",\")\n",
        "\n",
        "    precisionVec = []\n",
        "    recallVec = []\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1), np.min(Y1)])\n",
        "    gap = (end - start) / 200000\n",
        "\n",
        "    auprBase = 0.0\n",
        "    recallTemp = 1.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tp = np.sum(np.sum(X1 >= delta))  # / np.float(len(X1))\n",
        "        fp = np.sum(np.sum(Y1 >= delta))  # / np.float(len(Y1))\n",
        "        if tp + fp == 0:\n",
        "            continue\n",
        "        precision = tp / (tp + fp)\n",
        "        recall = tp / np.float(len(X1))\n",
        "        precisionVec.append(precision)\n",
        "        recallVec.append(recall)\n",
        "        auprBase += (recallTemp - recall) * precision\n",
        "        recallTemp = recall\n",
        "    auprBase += recall * precision\n",
        "\n",
        "    return auprBase\n",
        "\n",
        "\n",
        "def auprOut(dir_name):\n",
        "    # calculate the AUPR\n",
        "    cifar = np.loadtxt(\"%s/confidence_Base_In.txt\" % dir_name, delimiter=\",\")\n",
        "    other = np.loadtxt(\"%s/confidence_Base_Out.txt\" % dir_name, delimiter=\",\")\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1), np.min(Y1)])\n",
        "    gap = (end - start) / 200000\n",
        "\n",
        "    auprBase = 0.0\n",
        "    recallTemp = 1.0\n",
        "    for delta in np.arange(end, start, -gap):\n",
        "        fp = np.sum(np.sum(X1 < delta))  # / np.float(len(X1))\n",
        "        tp = np.sum(np.sum(Y1 < delta))  # / np.float(len(Y1))\n",
        "        if tp + fp == 0:\n",
        "            break\n",
        "        precision = tp / (tp + fp)\n",
        "        recall = tp / np.float(len(Y1))\n",
        "        auprBase += (recallTemp - recall) * precision\n",
        "        recallTemp = recall\n",
        "    auprBase += recall * precision\n",
        "\n",
        "    return auprBase\n",
        "\n",
        "\n",
        "def detection(dir_name):\n",
        "    # calculate the minimum detection error\n",
        "    cifar = np.loadtxt(\"%s/confidence_Base_In.txt\" % dir_name, delimiter=\",\")\n",
        "    other = np.loadtxt(\"%s/confidence_Base_Out.txt\" % dir_name, delimiter=\",\")\n",
        "\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1), np.min(Y1)])\n",
        "    gap = (end - start) / 200000\n",
        "\n",
        "    errorBase = 1.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tpr = np.sum(np.sum(X1 < delta)) / np.float(len(X1))\n",
        "        error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
        "        errorBase = np.minimum(errorBase, (tpr + error2) / 2.0)\n",
        "\n",
        "    return errorBase\n",
        "\n",
        "\n",
        "def metric(dir_name, task=\"OOD\"):\n",
        "    print(\"{}{:>34}\".format(task, \"Performance of Baseline detector\"))\n",
        "    fprBase = tpr95(dir_name)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"TNR at TPR 95%:\", (1 - fprBase) * 100))\n",
        "    aurocBase = auroc(dir_name)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"AUROC:\", aurocBase * 100))\n",
        "    errorBase = detection(dir_name)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"Detection acc:\", (1 - errorBase) * 100))\n",
        "    auprinBase = auprIn(dir_name)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"AUPR In:\", auprinBase * 100))\n",
        "    auproutBase = auprOut(dir_name)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"AUPR Out:\", auproutBase * 100))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xylXngoDs-Xl",
        "outputId": "317a05c2-aa15-40dd-c7b2-a8663a169d74"
      },
      "source": [
        "\n",
        "Iter_test = 100\n",
        "\n",
        "def mse(y, mean):\n",
        "    loss = tf.math.reduce_mean((y - mean) ** 2)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def generate_target():\n",
        "    f1 = open(\"%s/confidence_Base_In.txt\" % outf, \"w\")\n",
        "    test_loss = 0\n",
        "    for data, targets in in_test_ds:\n",
        "        current_mean = 0\n",
        "        for j in range(eval_iter):\n",
        "            mean, sigma = model(data)\n",
        "            current_mean = mean + current_mean\n",
        "            if j == 0:\n",
        "                Sigma = tf.expand_dims(sigma, 1)\n",
        "                Mean = tf.expand_dims(mean, 1)\n",
        "            else:\n",
        "                Sigma = tf.concat((Sigma, tf.expand_dims(sigma, 1)), axis=1)\n",
        "                Mean = tf.concat((Mean, tf.expand_dims(mean, 1)), axis=1)\n",
        "        current_mean = current_mean / eval_iter\n",
        "        loss = mse(targets, current_mean)\n",
        "        test_loss += loss.numpy()\n",
        "        Var_mean = tf.math.reduce_std(Mean,axis=-1)\n",
        "        for i in range(data.shape[0]):\n",
        "            soft_out = Var_mean[i].numpy()\n",
        "            f1.write(\"{}\\n\".format(-soft_out))\n",
        "\n",
        "    f1.close()\n",
        "\n",
        "    print(\"\\n Final RMSE: {}\".format(np.sqrt(test_loss / Iter_test) * target_scale))\n",
        "\n",
        "\n",
        "def generate_non_target():\n",
        "    f2 = open(\"%s/confidence_Base_Out.txt\" % outf, \"w\")\n",
        "    current_mean = 0\n",
        "    for j in range(eval_iter):\n",
        "        mean, sigma = model(X_out)\n",
        "        current_mean = mean + current_mean\n",
        "        if j == 0:\n",
        "            Sigma = tf.expand_dims(sigma, 1)\n",
        "            Mean = tf.expand_dims(mean, 1)\n",
        "        else:\n",
        "            Sigma = tf.concat((Sigma, tf.expand_dims(sigma, 1)), axis=1)\n",
        "            Mean = tf.concat((Mean, tf.expand_dims(mean, 1)), axis=1)\n",
        "\n",
        "    Var_mean = tf.math.reduce_std(Mean,axis=-1)\n",
        "    for i in range(X_out.shape[0]):\n",
        "        soft_out = Var_mean[i].numpy()\n",
        "        f2.write(\"{}\\n\".format(-soft_out))\n",
        "    f2.close()\n",
        "\n",
        "\n",
        "print(\"generate log from in-distribution data\")\n",
        "generate_target()\n",
        "print(\"generate log  from out-of-distribution data\")\n",
        "generate_non_target()\n",
        "print(\"calculate metrics for OOD\")\n",
        "metric(outf, \"OOD\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generate log from in-distribution data\n",
            "\n",
            " Final RMSE: 32.49121250125466\n",
            "generate log  from out-of-distribution data\n",
            "calculate metrics for OOD\n",
            "OOD  Performance of Baseline detector\n",
            "TNR at TPR 95%:            40.091%\n",
            "AUROC:                     78.295%\n",
            "Detection acc:             73.974%\n",
            "AUPR In:                   99.956%\n",
            "AUPR Out:                   0.717%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyp7Z8wTRgUR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}