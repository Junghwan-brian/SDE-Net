{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sdenet_torch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12o1lUVJfRyBHP9FUUxKDeQrRuJuNKx66",
      "authorship_tag": "ABX9TyMwXm1g4CRwCjMqerhY5BbR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junghwan-brian/SDE-Net/blob/master/colab_sdenet_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkrLwFczlHe6",
        "outputId": "347cfef5-97ec-4562-b128-0578f198b3e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Nov 18 04:52:10 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IpNAYl7J2V_"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from sklearn import datasets\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import torch.nn.init as init\n",
        "import math\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "from scipy import misc\n",
        "from numpy.linalg import inv\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_MSD():\n",
        "# Load the raw data.\n",
        "    num_attributes = 90\n",
        "    names = ['Year'] + ['Attr_{}'.format(i) for i in range(num_attributes)]\n",
        "    # YearPredictionMSD load file\n",
        "    df = pd.read_csv('drive/MyDrive/SDE-Net/YearPredictionMSD.txt', header=None, names=names)\n",
        "\n",
        "# Validate the data.\n",
        "    num_examples = 515345\n",
        "    assert len(df.columns) == num_attributes + 1\n",
        "    assert len(df) == num_examples\n",
        "    assert not df.isnull().values.any()\n",
        "\n",
        "\n",
        "# Train/test split. See \"Data Set Information\".\n",
        "    num_train = 463715\n",
        "    df = df.values\n",
        "    train = df[:num_train]\n",
        "    test = df[num_train:]\n",
        "    del df\n",
        "\n",
        "\n",
        "# Seperate inputs and outputs.\n",
        "    X_train, y_train = train[:, 1:], train[:, 0]\n",
        "    X_test, y_test = test[:, 1:], test[:, 0]\n",
        "    del train\n",
        "    del test\n",
        "    \n",
        "    standardize = StandardScaler().fit(X_train)\n",
        "    X_train = standardize.transform(X_train)\n",
        "    X_test = standardize.transform(X_test)\n",
        "\n",
        "    y_train1 = np.expand_dims(y_train, axis=1)\n",
        "    y_test1 = np.expand_dims(y_test , axis=1)\n",
        "\n",
        "    standardize2 = StandardScaler().fit(y_train1)\n",
        "    y_train = standardize2.transform(y_train1)\n",
        "    y_train = np.squeeze(y_train)\n",
        "\n",
        "    y_test1 = np.expand_dims(y_test, axis=1)\n",
        "    y_test = standardize2.transform(y_test1)\n",
        "    y_test = np.squeeze(y_test)\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def load_boston():\n",
        "    boston = datasets.load_boston()\n",
        "    x = boston.data\n",
        "    X = np.concatenate((x,x), axis=1)\n",
        "    for i in range(5):\n",
        "        X = np.concatenate((X,x), axis=1)\n",
        "    X = X[:,0:-1]\n",
        "    standardize = StandardScaler().fit(X)\n",
        "    X = standardize.transform(X)\n",
        "    return X\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(dataset):\n",
        "    if dataset == 'MSD':\n",
        "        X_train, y_train, X_test, y_test = load_MSD()\n",
        "        return X_train, y_train, X_test, y_test\n",
        "    if dataset == 'boston':\n",
        "        x = load_boston()\n",
        "        return x\n",
        "    \n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4KcYaPAkM7X",
        "outputId": "cdb81a36-302e-4488-abee-e0f0fccee650",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "__all__ = ['SDENet']\n",
        "\n",
        "class Drift(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Drift, self).__init__()\n",
        "        self.fc = nn.Linear(50, 50)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    def forward(self, t, x):\n",
        "        out = self.relu(self.fc(x))\n",
        "        return out    \n",
        "\n",
        "\n",
        "\n",
        "class Diffusion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Diffusion, self).__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(50, 100)\n",
        "        self.fc2 = nn.Linear(100, 1)\n",
        "    def forward(self, t, x):\n",
        "        out = self.relu(self.fc1(x))\n",
        "        out = self.fc2(out)\n",
        "        out = torch.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "    \n",
        "class SDENet(nn.Module):\n",
        "    def __init__(self, layer_depth):\n",
        "        super(SDENet, self).__init__()\n",
        "        self.layer_depth = layer_depth\n",
        "        self.downsampling_layers = nn.Linear(90, 50)\n",
        "        self.drift = Drift()\n",
        "        self.diffusion = Diffusion()\n",
        "        self.fc_layers = nn.Sequential(nn.ReLU(inplace=True), nn.Linear(50, 2))\n",
        "        self.deltat = 4./self.layer_depth\n",
        "        self.sigma = 0.5\n",
        "    def forward(self, x, training_diffusion=False):\n",
        "        out = self.downsampling_layers(x)\n",
        "        if not training_diffusion:\n",
        "            t = 0\n",
        "            diffusion_term = self.sigma*self.diffusion(t, out)\n",
        "            for i in range(self.layer_depth):\n",
        "                t = 4*(float(i))/self.layer_depth\n",
        "                out = out + self.drift(t, out)*self.deltat + diffusion_term*math.sqrt(self.deltat)*torch.randn_like(out).to(x)\n",
        "\n",
        "            final_out = self.fc_layers(out) \n",
        "            mean = final_out[:,0]\n",
        "            sigma = F.softplus(final_out[:,1])+1e-3\n",
        "            return mean, sigma\n",
        "            \n",
        "        else:\n",
        "            t = 0\n",
        "            final_out = self.diffusion(t, out.detach())  \n",
        "            return final_out\n",
        "\n",
        "def test():\n",
        "    model = SDENet(layer_depth=6)\n",
        "    return model  \n",
        " \n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = test()\n",
        "    num_params = count_parameters(model)\n",
        "    print(num_params)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miOMQkU-kM54"
      },
      "source": [
        "\n",
        "\n",
        "epochs=60\n",
        "lr = 1e-4\n",
        "lr2 = 0.01\n",
        "gpu = 0\n",
        "seed = 0\n",
        "droprate = 0.1\n",
        "decreasing_lr = [20]\n",
        "device = torch.device('cuda:' + str(gpu) if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "batch_size = 128\n",
        "Iter = 3622\n",
        "Iter_test = 403\n",
        "target_scale = 10.939756\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "if device == 'cuda':\n",
        "    cudnn.benchmark = True\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_dataset('MSD')\n",
        "\n",
        "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
        "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
        "y_train = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
        "y_test = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "net = SDENet(4)\n",
        "net = net.to(device)\n",
        "\n",
        "\n",
        "real_label = 0\n",
        "fake_label = 1\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer_F = optim.SGD([ {'params': net.downsampling_layers.parameters()}, {'params': net.drift.parameters()},\n",
        "{'params': net.fc_layers.parameters()}], lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "optimizer_G = optim.SGD([ {'params': net.diffusion.parameters()}], lr=lr2, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "def nll_loss(y, mean, sigma):\n",
        "    loss = torch.mean(torch.log(sigma**2)+(y-mean)**2/(sigma**2))\n",
        "    return loss\n",
        "def mse(y, mean):\n",
        "    loss = torch.mean((y-mean)**2)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def load_training(iternum):\n",
        "    x = X_train[iternum*batch_size:(iternum+1)*batch_size]\n",
        "    y = y_train[iternum*batch_size:(iternum+1)*batch_size]\n",
        "    return x, y\n",
        "\n",
        "def load_test(iternum):\n",
        "    x = X_test[iternum*batch_size:(iternum+1)*batch_size]\n",
        "    y = y_test[iternum*batch_size:(iternum+1)*batch_size]\n",
        "    return x, y\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    if epoch == 0:\n",
        "        net.sigma = 0.1\n",
        "    if epoch == 30:\n",
        "        net.sigma = 0.5\n",
        "    train_loss = 0\n",
        "    train_loss_in = 0\n",
        "    train_loss_out = 0\n",
        "    total = 0\n",
        "    for iternum in range(Iter):\n",
        "        inputs, targets = load_training(iternum)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer_F.zero_grad()\n",
        "        mean, sigma = net(inputs)\n",
        "        loss = nll_loss(targets, mean, sigma)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), 100.)\n",
        "        optimizer_F.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        label = torch.full((batch_size,1), real_label, device=device)\n",
        "        optimizer_G.zero_grad()\n",
        "        predict_in = net(inputs, training_diffusion=True)\n",
        "        label = label.to(torch.float64)\n",
        "        predict_in = predict_in.to(torch.float64)\n",
        "        loss_in = criterion(predict_in, label)\n",
        "        loss_in.backward()\n",
        "\n",
        "        label.fill_(fake_label)\n",
        "        inputs_out = 2*torch.randn(batch_size, 90, device = device)+inputs\n",
        "        predict_out = net(inputs_out, training_diffusion=True)\n",
        "        label = label.to(torch.float64)\n",
        "        predict_out = predict_out.to(torch.float64)\n",
        "        loss_out = criterion(predict_out, label)\n",
        "        \n",
        "        loss_out.backward()\n",
        "        train_loss_out += loss_out.item()\n",
        "        train_loss_in += loss_in.item()\n",
        "        optimizer_G.step()\n",
        "      \n",
        "    print('Train epoch:{} \\tLoss: {:.6f}| Loss_in: {:.6f}| Loss_out: {:.6f}'.format(epoch, train_loss/Iter, train_loss_in/Iter, train_loss_out/Iter))\n",
        "\n",
        "def test(epoch):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for iternum in range(Iter_test):\n",
        "            inputs, targets = load_test(iternum)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            current_mean = 0\n",
        "            for i in range(10):\n",
        "                mean, sigma = net(inputs)\n",
        "                current_mean = current_mean + mean\n",
        "            current_mean = current_mean/10\n",
        "            targets = targets.to(torch.float64)\n",
        "            current_mean = current_mean.to(torch.float64)\n",
        "            loss = mse(targets, current_mean)*target_scale\n",
        "            test_loss += loss.item()\n",
        "    \n",
        "    print('Test epoch:{} \\tLoss: {:.6f}'.format(epoch, np.sqrt(test_loss/Iter_test)))\n",
        "           \n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    if epoch in decreasing_lr:\n",
        "        for param_group in optimizer_F.param_groups:\n",
        "            param_group['lr'] *= droprate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvx1kNMTLG0A"
      },
      "source": [
        "\n",
        "os.mkdir(\"test\")\n",
        "\n",
        "def tpr95(dir_name, task = 'OOD'):\n",
        "    #calculate the falsepositive error when tpr is 95%\n",
        "    if task == 'OOD':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_In.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Out.txt'%dir_name, delimiter=',')\n",
        "    elif task == 'mis':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_Succ.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Err.txt'%dir_name, delimiter=',')\n",
        "\n",
        "\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1),np.min(Y1)])\n",
        "    gap = (end- start)/200000 # precision:200000\n",
        "\n",
        "    total = 0.0\n",
        "    fpr = 0.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tpr = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
        "        error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
        "        if tpr <= 0.96 and tpr >= 0.94:\n",
        "            fpr += error2\n",
        "            total += 1\n",
        "    if total == 0:\n",
        "        print('corner case')\n",
        "        fprBase = 1\n",
        "    else:\n",
        "        fprBase = fpr/total\n",
        "\n",
        "    return fprBase\n",
        "\n",
        "\n",
        "def auroc(dir_name, task = 'OOD'):\n",
        "    #calculate the AUROC\n",
        "    if task == 'OOD':\n",
        "        f1 = open('%s/Update_Base_ROC_tpr.txt'%dir_name, 'w')\n",
        "        f2 = open('%s/Update_Base_ROC_fpr.txt'%dir_name, 'w')\n",
        "        cifar = np.loadtxt('%s/confidence_Base_In.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Out.txt'%dir_name, delimiter=',')\n",
        "    elif task == 'mis':\n",
        "        f1 = open('%s/Update_Base_ROC_tpr_mis.txt'%dir_name, 'w')\n",
        "        f2 = open('%s/Update_Base_ROC_fpr_mis.txt'%dir_name, 'w')\n",
        "        cifar = np.loadtxt('%s/confidence_Base_Succ.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Err.txt'%dir_name, delimiter=',')\n",
        "\n",
        "\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1),np.min(Y1)])\n",
        "    gap = (end- start)/200000\n",
        "\n",
        "    aurocBase = 0.0\n",
        "    fprTemp = 1.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tpr = np.sum(np.sum(X1 >= delta)) / np.float(len(X1))\n",
        "        fpr = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
        "        f1.write(\"{}\\n\".format(tpr))\n",
        "        f2.write(\"{}\\n\".format(fpr))\n",
        "        aurocBase += (-fpr+fprTemp)*tpr\n",
        "        fprTemp = fpr\n",
        "    f1.close()\n",
        "    f2.close()\n",
        "    return aurocBase\n",
        "\n",
        "def auprIn(dir_name, task ='OOD'):\n",
        "    #calculate the AUPR\n",
        "    if task == 'OOD':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_In.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Out.txt'%dir_name, delimiter=',')\n",
        "    elif task == 'mis':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_Succ.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Err.txt'%dir_name, delimiter=',')\n",
        "\n",
        "    precisionVec = []\n",
        "    recallVec = []\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1),np.min(Y1)])\n",
        "    gap = (end- start)/200000\n",
        "\n",
        "    auprBase = 0.0\n",
        "    recallTemp = 1.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tp = np.sum(np.sum(X1 >= delta)) #/ np.float(len(X1))\n",
        "        fp = np.sum(np.sum(Y1 >= delta)) #/ np.float(len(Y1))\n",
        "        if tp + fp == 0: continue\n",
        "        precision = tp / (tp + fp)\n",
        "        recall = tp/ np.float(len(X1))\n",
        "        precisionVec.append(precision)\n",
        "        recallVec.append(recall)\n",
        "        auprBase += (recallTemp-recall)*precision\n",
        "        recallTemp = recall\n",
        "    auprBase += recall * precision\n",
        "\n",
        "    return auprBase\n",
        "\n",
        "def auprOut(dir_name, task = 'OOD'):\n",
        "    #calculate the AUPR\n",
        "    if task == 'OOD':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_In.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Out.txt'%dir_name, delimiter=',')\n",
        "    elif task == 'mis':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_Succ.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Err.txt'%dir_name, delimiter=',')\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1),np.min(Y1)])\n",
        "    gap = (end- start)/200000\n",
        "\n",
        "    auprBase = 0.0\n",
        "    recallTemp = 1.0\n",
        "    for delta in np.arange(end, start, -gap):\n",
        "        fp = np.sum(np.sum(X1 < delta)) #/ np.float(len(X1))\n",
        "        tp = np.sum(np.sum(Y1 < delta)) #/ np.float(len(Y1))\n",
        "        if tp + fp == 0: break\n",
        "        precision = tp / (tp+fp)\n",
        "        recall = tp/np.float(len(Y1))\n",
        "        auprBase += (recallTemp-recall)*precision\n",
        "        recallTemp = recall\n",
        "    auprBase += recall * precision\n",
        "\n",
        "    return auprBase\n",
        "\n",
        "def detection(dir_name, task = 'OOD'):\n",
        "    #calculate the minimum detection error\n",
        "    if task == 'OOD':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_In.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Out.txt'%dir_name, delimiter=',')\n",
        "    elif task == 'mis':\n",
        "        cifar = np.loadtxt('%s/confidence_Base_Succ.txt'%dir_name, delimiter=',')\n",
        "        other = np.loadtxt('%s/confidence_Base_Err.txt'%dir_name, delimiter=',')\n",
        "\n",
        "    Y1 = other\n",
        "    X1 = cifar\n",
        "    end = np.max([np.max(X1), np.max(Y1)])\n",
        "    start = np.min([np.min(X1),np.min(Y1)])\n",
        "    gap = (end- start)/200000\n",
        "\n",
        "    errorBase = 1.0\n",
        "    for delta in np.arange(start, end, gap):\n",
        "        tpr = np.sum(np.sum(X1 < delta)) / np.float(len(X1))\n",
        "        error2 = np.sum(np.sum(Y1 > delta)) / np.float(len(Y1))\n",
        "        errorBase = np.minimum(errorBase, (tpr+error2)/2.0)\n",
        "\n",
        "    return errorBase\n",
        "\n",
        "def metric(dir_name, task):\n",
        "    print(\"{}{:>34}\".format(task, \"Performance of Baseline detector\"))\n",
        "    fprBase = tpr95(dir_name, task)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"TNR at TPR 95%:\", (1-fprBase)*100))\n",
        "    aurocBase = auroc(dir_name, task)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"AUROC:\",aurocBase*100))\n",
        "    errorBase = detection(dir_name,task)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"Detection acc:\",(1-errorBase)*100))\n",
        "    auprinBase = auprIn(dir_name,task)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"AUPR In:\",auprinBase*100))\n",
        "    auproutBase = auprOut(dir_name,task)\n",
        "    print(\"{:20}{:13.3f}%\".format(\"AUPR Out:\",auproutBase*100))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxU2xxmGIq2R",
        "outputId": "bc069031-5222-4111-ba39-e50f32805cfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "# Training settings\n",
        "eval_iter = 10\n",
        "seed = 0\n",
        "gpu = 0\n",
        "outf = 'test/'\n",
        "droprate = 0.1\n",
        "\n",
        "\n",
        "device = torch.device('cuda:' + str(gpu) if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Random Seed: \", seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "X_train, y_train, X_test, y_test = load_dataset('MSD')\n",
        "\n",
        "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
        "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
        "y_train = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
        "y_test = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
        "Iter_test = 100\n",
        "batch_size = 512\n",
        "target_scale = 10.939756\n",
        "\n",
        "    \n",
        "\n",
        "X_out = load_dataset('boston')\n",
        "X_out = torch.from_numpy(X_out).type(torch.FloatTensor)\n",
        "\n",
        "\n",
        "def mse(y, mean):\n",
        "    loss = torch.mean((y-mean)**2)\n",
        "    return loss\n",
        "\n",
        "def load_test(iternum):\n",
        "    x = X_test[iternum*batch_size:(iternum+1)*batch_size]\n",
        "    y = y_test[iternum*batch_size:(iternum+1)*batch_size]\n",
        "    return x, y\n",
        "\n",
        "def generate_target():\n",
        "    net.eval()  \n",
        "    test_loss = 0\n",
        "    total = 0\n",
        "    f1 = open('%s/confidence_Base_In.txt'%outf, 'w')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for iternum in range(Iter_test):\n",
        "            data, targets = load_test(iternum)\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            current_mean = 0\n",
        "            temp = 0\n",
        "            for j in range(eval_iter):\n",
        "                mean, sigma  = net(data)\n",
        "                current_mean = mean + current_mean\n",
        "                if j == 0:\n",
        "                    Sigma = torch.unsqueeze(sigma,1)\n",
        "                    Mean = torch.unsqueeze(mean,1)\n",
        "                else:\n",
        "                    Sigma = torch.cat((Sigma, torch.unsqueeze(sigma,1)),dim=1)\n",
        "                    Mean = torch.cat((Mean, torch.unsqueeze(mean,1)),dim=1)\n",
        "            current_mean = current_mean/eval_iter\n",
        "            loss = mse(targets, current_mean)\n",
        "            test_loss += loss.item()\n",
        "            Var_mean = Mean.std(dim=1)\n",
        "            for i in range(data.size(0)):\n",
        "                soft_out = Var_mean[i].item()\n",
        "                f1.write(\"{}\\n\".format(-soft_out))\n",
        "\n",
        "    f1.close()\n",
        "\n",
        "    print('\\n Final RMSE: {}'.format(np.sqrt(test_loss/Iter_test)*target_scale))\n",
        "\n",
        "def generate_non_target():\n",
        "    net.eval()\n",
        "\n",
        "    f2 = open('%s/confidence_Base_Out.txt'%outf, 'w')\n",
        "    with torch.no_grad():\n",
        "        data  = X_out.to(device)\n",
        "        current_mean = 0\n",
        "        temp = 0\n",
        "        for j in range(eval_iter):\n",
        "            mean, sigma  = net(data)\n",
        "            if j == 0:\n",
        "                Sigma = torch.unsqueeze(sigma,1)\n",
        "                Mean = torch.unsqueeze(mean,1)\n",
        "            else:\n",
        "                Sigma = torch.cat((Sigma, torch.unsqueeze(sigma,1)),dim=1)\n",
        "                Mean = torch.cat((Mean, torch.unsqueeze(mean,1)),dim=1)\n",
        "            current_mean = mean + current_mean\n",
        "        current_mean = current_mean/eval_iter\n",
        "        Var_mean = Mean.std(dim=1)\n",
        "        for i in range(data.size(0)):\n",
        "            soft_out = Var_mean[i].item()\n",
        "            f2.write(\"{}\\n\".format(-soft_out))\n",
        "    f2.close()\n",
        "\n",
        "print('generate log from in-distribution data')\n",
        "generate_target()\n",
        "print('generate log  from out-of-distribution data')\n",
        "generate_non_target()\n",
        "print('calculate metrics for OOD')\n",
        "metric(outf, 'OOD')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  0\n",
            "generate log from in-distribution data\n",
            "\n",
            " Final RMSE: 8.74443924525597\n",
            "generate log  from out-of-distribution data\n",
            "calculate metrics for OOD\n",
            "OOD  Performance of Baseline detector\n",
            "TNR at TPR 95%:            57.926%\n",
            "AUROC:                     83.891%\n",
            "Detection acc:             79.920%\n",
            "AUPR In:                   99.719%\n",
            "AUPR Out:                  18.478%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}